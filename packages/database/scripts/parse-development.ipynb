{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Parse development book\n",
    "\n",
    "### Notebook purpose\n",
    "This notebook is development space for python parse.ts replacement and upgrade.\n",
    "It reads specified google sheets and output actants.json file, which can be imported to inkVisitor RethinkDB.py\n",
    "\n",
    "The aggreated doc for dev: https://docs.google.com/document/d/1ga6R_9TWAQlXE9XqPE_qZ2S8S8aVW2A7n8SuYpSXnoI/edit#heading=h.q9ntf0ofam2u\n",
    "The holy schema: https://app.diagrams.net/#G1bKvqEKr6JzPWryVg-vYudQy_4KvuzHS9\n",
    "\n",
    "### JSon schemas for the actants\n",
    " * are created from *.ts files through typescript-jschema commandline utility\n",
    " * the schemas are trasformed as python Classes through warlock library\n",
    "\n",
    "\n",
    "### Principles of the parsing operations\n",
    " * There are google sheet dataset, which need to be transformed to json format according to inkVisior datamodel\n",
    " * inkVisitor holds datamodel in the code, the typescript classes (see /shared/types...), or the holy schema\n",
    " * DZ created in input table parsing instructions\n",
    "   * the first 5 rows contain instructions\n",
    "   * if the keywords contain ? character, they are ignored (it is work in progress from DZ)\n",
    "\n",
    "#### Parsing instructions\n",
    " * explicit set of keywords\n",
    "   * discard\n",
    "     * ...\n",
    "   * inside\n",
    "     * the value in the column should be straitforwadly made part of the entity object, the exists update_generic method or update_fieldName mehthods for fields with custom but generally applicable logic\n",
    "   * propvalue\n",
    "     * for making so called\n",
    "          * statement or property statment (old A0093 has relation)\n",
    "     * it sits in the entity \"props\" attribute, it has IProp class\n",
    "   * special\n",
    "     * fully custom method for the parsing behavior\n",
    "     *\n",
    "\n",
    "### Parsing process\n",
    " 1. Prepare data model entities classes (from typescript inkvisitor classes -> json schema ->  python classes)\n",
    " 2. Load and wrangle all input data\n",
    " 3. Process headers of the tables for parsing instructions\n",
    " 4. Process tables row by row, column by column\n",
    " 5. Save as json file (which can be imported to the inkVisitor RethinkDB)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dev updates: April\n",
    "\n",
    "## Concepts\n",
    "  * DONE wordnet_synset_id\n",
    "    * wordnet_id bounded to resource object R0067\n",
    "\n",
    "## Texts\n",
    " * DONE special_creation_event_id\n",
    "   * instructions:\n",
    "     * Create entities in this col. as new E entities, with (1) the value here as legacy_id, (2) assign (as usual) a new \"hash\" ID from the db, (3) label of this E: see next col., (4) logical type \"definite\" (default), (5) label language \"English\", (6) status \"approved\", and (7) attach to any of those Es the metaprop \"(has) - C0565 \"class\" - C2642 \"creation\" (to instantiate the event to its event type = event class).\n",
    "   * creation_event_label in standalone field\n",
    "     * this will trigger the operation\n",
    "   * lot of proptype and propvalue with non-standard schema: new branch of interpreting\n",
    "\n",
    "## Manuscripts\n",
    " * special_repository_label\n",
    "   * i:\n",
    "     * For each non-empty, non-NA, non-NS row: (1) generate L entity with label = value in this col., status = \"approved\", entity logical type = \"definite\", label language = value in the next col. (repository_label_language); (2) append to this L entity a metaprop (has) - C0565 \"class\" - C2646 \"archive or library\", and (3) under the O entity representing the row (i.e. the physical manuscript), add a metaprop which will relate this O to this L entity (repository) through the relation: O - (has) - C2645 \"repository\" - L in this col.\n",
    "   * repository_label_language\n",
    "     * i:\n",
    "       * Use this value as label language value of the repository L entity.\n",
    " * DONE creation_event_id\n",
    "   * creation_event_label\n",
    "   * i:\n",
    "     * Create entities in this col. as new E entities, with (1) the value here as legacy_id, (2) assign (as usual) a new \"hash\" ID from the db, (3) label of this E: see next col., (4) logical type \"definite\" (default), (5) label language \"English\", (6) status \"approved\", and (7) attach to any of those Es the metaprop \"(has) - C0565 \"class\" - C2642 \"creation\" (to instantiate the event to its event type = event class).\n",
    " * reproduction_online_url\n",
    "   * instructions:\n",
    "     * If non-empty, non-NA, (1) generate an R entity with label \"Reproduction of \" + label of the MS (i.e. value in the B column, status = \"approved\", label-language = \"English\", url = the URL sitting under the hyperlink value in this cell, and (2) add metaprop to the O entity represented by this row: O - (has) - C1199 \"digital reproduction\" - the R entity here generated."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Input variables"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_tables = [\"texts\", \"manuscripts\", \"resources\", \"actions\" , \"concepts\"]\n",
    "\n",
    "#                   sheet_name,  code, header_in_row\n",
    "input_sheets = {\n",
    "    \"texts\" : (\"Texts\",\"13eVorFf7J9R8YzO7TmJRVLzIIwRJS737r7eFbH1boyE\", 5), #https://docs.google.com/spreadsheets/d/13eVorFf7J9R8YzO7TmJRVLzIIwRJS737r7eFbH1boyE/edit#gid=2056508047\n",
    "    \"manuscripts\" : (\"Manuscripts\", \"13eVorFf7J9R8YzO7TmJRVLzIIwRJS737r7eFbH1boyE\", 5),\n",
    "    \"resources\" : (\"Resources\", \"13eVorFf7J9R8YzO7TmJRVLzIIwRJS737r7eFbH1boyE\", 5),\n",
    "    \"actions\" :  (\"Statements\",\"1vzY6opQeR9hZVW6fmuZu2sgy_izF8vqGGhBQDxqT_eQ\", 5), # https://docs.google.com/spreadsheets/d/1vzY6opQeR9hZVW6fmuZu2sgy_izF8vqGGhBQDxqT_eQ/edit#gid=0\n",
    "    \"concepts\" : (\"Concepts\",\"1nSqnN6cjtdWK-y6iKZlJv4iGdhgtqkRPus8StVgExP4\", 5) # https://docs.google.com/spreadsheets/d/1nSqnN6cjtdWK-y6iKZlJv4iGdhgtqkRPus8StVgExP4/edit#gid=0\n",
    "}\n",
    "\n",
    "table_to_entity = {\n",
    "    \"concepts\" : \"IConcept\",\n",
    "    \"resources\" : \"IResource\",\n",
    "    \"texts\" : \"ITerritory\",\n",
    "    \"manuscripts\" : \"IObject\",\n",
    "    \"actions\" :  \"IAction\",\n",
    "}\n",
    "\n",
    "root_sheet_url = \"https://docs.google.com/spreadsheets/d/\"\n",
    "google_api_dotenv_path = \"../env/.env.googleapi\"  # contains google api specs for sheet access with Dator\n",
    "schema_path = '../schemas/' # path for dir with schemas\n",
    "json_schemas = {}  # holder for schemas, so they can be used for jsonschema validate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.run(\"python generate-json-schemas.py\", shell=True,capture_output=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, warlock, json\n",
    "from datetime import datetime\n",
    "import time\n",
    "from jsonschema import validate\n",
    "import dissinetpytools.dator as dator\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from shutil import copyfile\n",
    "\n",
    "import uuid\n",
    "\n",
    "def get_uuid_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "def is_valid_uuid(val):\n",
    "    try:\n",
    "        uuid.UUID(str(val))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "# type hinting\n",
    "from collections.abc import Sequence, Callable\n",
    "from typing import List, Dict, Tuple\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialisation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "load_dotenv(google_api_dotenv_path) # fills os.environ['GDRIVE_API_CREDENTIALS']\n",
    "d = dator.Dator(loglevel=10, print_log_online=True, cache=True, project_name=\"inkvisitor-import\") # expects 'GDRIVE_API_CREDENTIALS' in the global system variables (os.environ)\n",
    "d.google_authenticate()\n",
    "logger = d.logger"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read all schemas inside and warlock them as globally available classes\n",
    "schema_filenames = os.listdir(schema_path)\n",
    "json_classes = {}\n",
    "for schema in schema_filenames:\n",
    "    name = schema.split(\".\")[0]\n",
    "    file_handler = open(schema_path + schema,\"r\")\n",
    "    schema_json = json.load(file_handler)\n",
    "    json_schemas[name] = schema_json\n",
    "    globals()[name] = warlock.model_factory(schema_json)\n",
    "    json_classes[name] = globals()[name]\n",
    "    logger.info(\"Class \" + name + \" available.\")\n",
    "\n",
    "logger.info(f\"There are {len(json_classes.keys())} json classes available ({' '.join(json_classes.keys())}).\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# factory for making entity objects, contains defaults with \"prerequisities\"\n",
    "from datetime import datetime\n",
    "import_note = \"Import batch [development] \" + str(datetime.now())\n",
    "\n",
    "class InkVisitorJSONObjectFactory:\n",
    "\n",
    "    classes = json_classes\n",
    "\n",
    "    json_class_defaults = {\n",
    "        'IAction':{\n",
    "            'class':'A', 'id':'','legacyId':'', 'label':'', 'language':'', 'detail':'','data':{'entities':{'a1':[],'a2':[],'s':[]},'valencies':{'a1':'','a2':'','s':''}}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'IConcept':{\n",
    "            'class':'C', 'id':'', 'legacyId':'','label':'', 'language':'', 'detail':'','data':{}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'IValue':{\n",
    "            'class':'V', 'id':'', 'label':'', 'language':'', 'detail':'','data':{'logicalType':'1'}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'IProp':{\n",
    "            'bundleEnd':False,'bundleStart':False, 'certainty':'1', 'children':[], 'elvl':'1',  'id':'', 'logic':'1', 'mood':[], 'moodvariant':'1', 'bundleOperator':'a', 'type': {'id':'','elvl':'1','logic':'1','partitivity':'1','virtuality':'1'},'value':{'id':'','elvl':'1', 'logic':'1', 'partitivity':'1', 'virtuality':'1'}\n",
    "        },\n",
    "        'IResource':{\n",
    "             'class':'R', 'id':'', 'label':'', 'language':'', 'detail':'','data':{'url':'','partValueBaseURL':'','partValueLabel':''}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'IObject':{\n",
    "             'class':'O', 'id':'', 'label':'', 'language':'', 'detail':'','data':{'logicalType':'1'}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'IStatement':{\n",
    "             'class':'S', 'id':'', 'label':'', 'language':'', 'detail':'','data':{'actants':[], 'actions':[], 'tags':[],'territory': {'id':'','order':0}, 'text':''}, 'props':[], 'notes':[], 'status':'1','references':[]\n",
    "        },\n",
    "        'ITerritory':{\n",
    "            'class':'T', 'id':'', 'legacyId':'','label':'', 'language':'', 'detail':'','data':{'parent':{ \"id\": \"T0\", \"order\": 0 }}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'ILocation':{\n",
    "            'class':'L', 'id':'', 'label':'', 'language':'', 'detail':'','data':{'logicalType':'1'}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'IEvent':{\n",
    "            'class':'E', 'id':'', 'label':'', 'language':'', 'detail':'','data':{'logicalType':'1'}, 'props':[], 'notes':[], 'status':'1', 'references':[]\n",
    "        },\n",
    "        'IReference':{\n",
    "           'id':'','resource':'','value':''\n",
    "        },\n",
    "        'IAudit':{\n",
    "           'id':'','entityId':'','user':'','date':'','changes':{}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        for key, item in type(self).json_class_defaults.items():\n",
    "            if 'notes' in item and  len(item['notes']) == 0:\n",
    "                item['notes'].append(import_note)\n",
    "\n",
    "    def make(self, entity_name, override_object=None):\n",
    "        if override_object is None:\n",
    "            override_object = {}\n",
    "        object = type(self).json_class_defaults[entity_name]\n",
    "        object.update(override_object)\n",
    "        return type(self).classes[entity_name](deepcopy(object))\n",
    "\n",
    "    def validate_defaults(self):\n",
    "        for e in self.json_class_defaults:\n",
    "            test = self.make(e, self.json_class_defaults[e])\n",
    "            d.logger.info(f\"Class {e} validated.\")\n",
    "\n",
    "\n",
    "IOF = InkVisitorJSONObjectFactory()\n",
    "IOF.validate_defaults()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load input datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# empty value unifier\n",
    "def unify_empty_value(df: pd.DataFrame, empty_values=None, unified_empty_value =''):\n",
    "    if empty_values is None:\n",
    "        empty_values = ['NA', \"#N/A\", \"#VALUE!\"]\n",
    "    for naner in empty_values:\n",
    "        df = df.replace(naner,unified_empty_value)\n",
    "    df. fillna(unified_empty_value, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# load all input tables\n",
    "tables = {}\n",
    "header_infos = {}\n",
    "entity_ids = {}\n",
    "for key, sheet in input_sheets.items():\n",
    "    logger.info(f\"Calling for {key} with sheet_name {sheet[0]}.\")\n",
    "    tables[key], header_infos[key] = d.load_df_from_gsheet(sheet[0],root_sheet_url + sheet[1], sheet[0], fromCache=False, header_in_row=sheet[2], clean=True, fillna=True, cleanByColumn=\"label\", parse_hyperlink_formulas=True, numerize=False)\n",
    "    tables[key] = unify_empty_value(tables[key])\n",
    "    header_infos[key] = unify_empty_value(header_infos[key])\n",
    "\n",
    "    # code for legacyId copy and uuid creation\n",
    "    # tables[key]['legacyId'] = tables[key]['id'].copy()\n",
    "    tables[key].insert(2, 'legacyId', tables[key]['id'].copy())\n",
    "\n",
    "    # inform instructive header about the new column and what to do with it\n",
    "    header_infos[key]['legacyId'] = \"\"\n",
    "    header_infos[key].at[3,'legacyId'] = \"inside\"\n",
    "    tables[key]['id'] = tables[key].apply(lambda x: get_uuid_id(), axis=1)  # generate unique id for each row\n",
    "    # make id dictionaries (it is much faster to search for keys there in legacyId>id retrievals)\n",
    "    ed = tables[key][[\"legacyId\",\"id\"]].set_index(\"legacyId\")\n",
    "    entity_ids[key] = ed[\"id\"].to_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# making empty table of values\n",
    "tables['values'] = pd.DataFrame(columns=['id','value','origin'])\n",
    "tables['locations'] = pd.DataFrame(columns=['id','value','origin','legacyId'])\n",
    "tables['events'] = pd.DataFrame(columns=['id','value','origin','legacyId'])\n",
    "tables['props'] = pd.DataFrame(columns=['id','type','type_id','value','value_id','original_field','origin','entityId','legacyId'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "header_infos['texts']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tables['manuscripts'][tables['manuscripts']['reproduction_online_url']==\"link\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# manual corrections in the input data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Responsible editors\n",
    "\n",
    "#### Data\n",
    "Only concepts have \"editors\" filled.\n",
    "\n",
    "#### Specification\n",
    "\n",
    "Only entities have audits, i.e. the prop object, the ref object don't. (Could be wrong?)\n",
    "\n",
    "**Rationale:**  Each entity will have 2 audit creation records. One with the user \"import\", second with the user \"responsible editor.\n",
    "\n",
    "Implementaions possiblities:\n",
    "  - make the audits during the parsing : the way\n",
    "  - make the audits afer the parsing is done (for secondary created entities, I will not have the context, and will be problem to find an responsible editor\n",
    "  -"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tables['concepts']['editor'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the names in the input tables  vs their user ids\n",
    "# the tuser are imported through the user.json file\n",
    "\n",
    "editors = {\"import\":\"0\", \"David Zbíral\":\"100\", \"Robert Shaw\":\"101\", \"Davor Salihović\":\"102\", \"Katia Riccardo\":\"103\"}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Declaration of controlling classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "additional_entities = []\n",
    "audits = []\n",
    "\n",
    "# for controlling entity and mapping of its fields\n",
    "class EntityMapper:\n",
    "\n",
    "    # simple inside values mapping from input_values in gsheets to inkVisitor enums\n",
    "    # field: { FROM  : TO }\n",
    "    enum_mapper = {'language': {\"English\":\"eng\",\"Latin\":\"lat\",\"Occitan\":\"oci\",\"Middle English\":\"enm\",\"Czech\":\"ces\",\"Italian\":\"ita\",\"French\":\"fra\",\"German\":\"deu\"},\"status\":{\"approved\":\"1\",\"pending\":\"0\",\"discouraged\":\"2\",\"warning\":\"3\"}}\n",
    "    # status  Pending = \"0\",   Approved = \"1\",  Discouraged = \"2\",  Warning = \"3\",\n",
    "    valid_entity_classes = ['A','C','E','O','R','T','P','G','S','L','NULL']\n",
    "\n",
    "\n",
    "    IOF = InkVisitorJSONObjectFactory()\n",
    "\n",
    "    def __init__(self, entity_type, data_row, logger = d.logger):\n",
    "        self.entity =  type(self).IOF.make(entity_type)\n",
    "        self.logger = logger\n",
    "        self.debug = True\n",
    "        self.data_row = data_row\n",
    "\n",
    "\n",
    "    def make_prop_object(self,prop_type_id, prop_value_id):\n",
    "        prop_object = IOF.make('IProp')\n",
    "        prop_object['id'] = get_uuid_id()\n",
    "        prop_object['type']['id'] = prop_type_id\n",
    "        prop_object['value']['id'] = prop_value_id\n",
    "        return prop_object\n",
    "\n",
    "    def make_ref_object(self, ref_resource_id, ref_value_id):\n",
    "        ref_object = IOF.make('IReference')\n",
    "        ref_object['id'] = get_uuid_id()\n",
    "        ref_object['resource'] = ref_resource_id\n",
    "        ref_object['value'] = ref_value_id\n",
    "        return ref_object\n",
    "\n",
    "    def get_entity_id(self,entity_string, origin = \"\"):\n",
    "        id = \"\"\n",
    "        # logger.info(f\"Getting entity string {entity_string} in {origin}\")\n",
    "        try:\n",
    "            if entity_string.startswith(\"C\") and entity_string[1:].isnumeric():\n",
    "                # id = tables['concepts'].loc[tables['concepts']['legacyId'] == entity_string,'id'].values[0]\n",
    "                id = entity_ids[\"concepts\"][entity_string]\n",
    "            elif entity_string.startswith(\"~V~\"):\n",
    "                ventity = self.make_ventity(entity_string, origin=origin)\n",
    "                id = ventity['id']\n",
    "            elif entity_string.startswith(\"M\") and entity_string[1:].isnumeric():\n",
    "                # id = tables['manuscripts'].loc[tables['manuscripts']['legacyId'] == entity_string,'id'].values[0]\n",
    "                id = entity_ids[\"manuscripts\"][entity_string]\n",
    "            elif entity_string.startswith(\"A\") and entity_string[1:].isnumeric():\n",
    "                # id = tables['actions'].loc[tables['actions']['legacyId'] == entity_string,'id'].values[0]\n",
    "                id = entity_ids[\"actions\"][entity_string]\n",
    "            elif entity_string.startswith(\"R\") and entity_string[1:].isnumeric():\n",
    "                # id = tables['resources'].loc[tables['resources']['legacyId'] == entity_string,'id'].values[0]\n",
    "                id = entity_ids[\"resources\"][entity_string]\n",
    "            elif entity_string.startswith(\"T\") and entity_string[1:].isnumeric():\n",
    "                # id = tables['resources'].loc[tables['resources']['legacyId'] == entity_string,'id'].values[0]\n",
    "                id = entity_ids[\"texts\"][entity_string]\n",
    "\n",
    "            elif is_valid_uuid(entity_string):\n",
    "                id = entity_string\n",
    "\n",
    "        except IndexError as E:\n",
    "            logger.info(f\"Cannot get entity id from entity string {entity_string} in {origin}. {E}\")\n",
    "\n",
    "        if id != \"\" and isinstance(id, str):\n",
    "            # logger.info(f\"Got entity id {id} from entity string {entity_string} in {origin}\")\n",
    "            return id\n",
    "        else:\n",
    "            logger.error(f\"Cannot get entity id from {entity_string}.\")\n",
    "            raise Exception(f\"Cannot get entity id from {entity_string}.\")\n",
    "\n",
    "    def make_ventity(self, value_string, origin=\"\"):\n",
    "        # logger.info(f\"Generating ventity from {value_string}.\")\n",
    "        # generate value entity object...\n",
    "        ventity = IOF.make('IValue')\n",
    "        ventity['id'] = get_uuid_id()\n",
    "        ventity['label'] = value_string.replace(\"~V~\",\"\")\n",
    "\n",
    "        if self.debug:\n",
    "            ventity['notes'].append(origin)\n",
    "\n",
    "        # register ventity\n",
    "        tables['values'] = tables['values'].append({'id':ventity['id'] ,'value':ventity['label'],\"origin\":origin},ignore_index=True )\n",
    "        additional_entities.append(ventity)\n",
    "\n",
    "        # logger.info(f\"Ventity id={ventity['id']} generated.\")\n",
    "\n",
    "        # create audit record\n",
    "        self.create_audit_record(entity_id=ventity['id'], object=ventity)\n",
    "\n",
    "        return ventity\n",
    "\n",
    "    def make_rentity(self, label, url = \"\", origin=\"\"):\n",
    "        # logger.info(f\"Generating rentity from {value_string}.\")\n",
    "        # generate resource entity object...\n",
    "        rentity = IOF.make('IResource')\n",
    "        rentity['id'] = get_uuid_id()\n",
    "        rentity['label'] = label\n",
    "        rentity['data']['url'] = url\n",
    "\n",
    "        if self.debug:\n",
    "            rentity['notes'].append(origin)\n",
    "\n",
    "        # register rentity\n",
    "        tables['resources'] = tables['resources'].append({'id':rentity['id'] ,'value':rentity['label'],\"origin\":origin}, ignore_index=True )\n",
    "        additional_entities.append(rentity)\n",
    "\n",
    "        # logger.info(f\"Rentity id={rentity['id']} generated.\")\n",
    "        return rentity\n",
    "\n",
    "\n",
    "    def make_eentity(self, label, legacyId, url = \"\", origin=\"\"):\n",
    "\n",
    "        eentity = IOF.make('IEvent')\n",
    "        eentity['id'] = get_uuid_id()\n",
    "        eentity['label'] = label\n",
    "        eentity['legacyId'] = legacyId\n",
    "\n",
    "        if self.debug:\n",
    "            eentity['notes'].append(origin)\n",
    "\n",
    "        # register event\n",
    "        tables['events'] = tables['events'].append({'id':eentity['id'] ,'value':eentity['label'],\"origin\":origin, \"legacyId\":legacyId}, ignore_index=True )\n",
    "        additional_entities.append(eentity)\n",
    "\n",
    "        return eentity\n",
    "\n",
    "    def make_lentity(self, label, legacyId=\"\", url = \"\", origin=\"\"):\n",
    "\n",
    "        lentity = IOF.make('ILocation')\n",
    "        lentity['id'] = get_uuid_id()\n",
    "        lentity['label'] = label\n",
    "        lentity['legacyId'] = legacyId\n",
    "\n",
    "        if self.debug:\n",
    "            lentity['notes'].append(origin)\n",
    "\n",
    "        # register lentity\n",
    "        tables['locations'] = tables['locations'].append({'id':lentity['id'] ,'value':lentity['label'],\"origin\":origin,\"legacyId\":legacyId}, ignore_index=True )\n",
    "        additional_entities.append(lentity)\n",
    "\n",
    "        return lentity\n",
    "\n",
    "    # interprets prop_type (should be always concept or resource) and input_value (should be concept or value string)\n",
    "    # get ids of the prop_type and prop_value (possibly creates and register values object)\n",
    "    # make iProp object\n",
    "    # puts iProp object into the entity props property\n",
    "    def hook_prop_object(self, prop_type, input_value, prop_source=\"\",  origin=\"\", prop_source_id =\"\", field_name=\"\", prop_source_field = \"\"):\n",
    "\n",
    "        # allowed entities in type\n",
    "        assert \"C\" in prop_type[0] or \"R\" in prop_type[0], f\"Prop type unknown, C or R string entity expected? {prop_type}, {origin}\"\n",
    "\n",
    "        # allowed entities in input\n",
    "        allowed_strict_entities = ['C','M','A','E','L','R','S','V','O','T'] # should be followed by numbers\n",
    "        allowed_free_string_entities = ['~V~']\n",
    "\n",
    "        # checking input_value\n",
    "        if not is_valid_uuid(input_value):\n",
    "            if any(input_value.startswith(c)for c in allowed_strict_entities):\n",
    "                # check for numbers\n",
    "                if not input_value[1:].isnumeric():\n",
    "                    input_value = \"~V~\"+input_value\n",
    "            elif not any(input_value.startswith(c)for c in allowed_free_string_entities):\n",
    "                input_value = \"~V~\"+input_value\n",
    "\n",
    "        # TODO this is useless now, because the line above will make ~V~ from everything unknown\n",
    "        assert any(input_value.startswith(c)  for c in allowed_strict_entities) or any(input_value.startswith(c)  for c in allowed_free_string_entities) or is_valid_uuid(input_value), f\"Prop value unknown, C string or V string entity expected, or valid uuid.{input_value}, {origin}\"\n",
    "\n",
    "        prop_type_id = self.get_entity_id(prop_type, origin=origin)\n",
    "        if not is_valid_uuid(input_value):\n",
    "            prop_value_id = self.get_entity_id(input_value, origin=origin)\n",
    "        else:\n",
    "            prop_value_id = input_value\n",
    "        # logger.info(f\"{prop_type_id}, {prop_value_id}\")\n",
    "\n",
    "        # make IProp object\n",
    "        prop_object = self.make_prop_object(prop_type_id, prop_value_id)\n",
    "\n",
    "        # register prop object\n",
    "        tables['props'] = tables['props'].append({'id':prop_object['id'] , 'type_id':prop_type_id,'value_id':prop_value_id,'type':prop_type,'value':input_value, \"original_field\":field_name, \"origin\":origin, 'entityId':self.entity['id'], 'legacyId':self.entity['legacyId']}, ignore_index=True)\n",
    "\n",
    "        if prop_source_field != \"\":\n",
    "            self.hook_2ndprop_into_props(prop_object,prop_source_field = prop_source_field)\n",
    "        elif prop_source_id !=\"\": # means propvalue_2nd\n",
    "            self.hook_2ndprop_into_props(prop_object,prop_source_id = prop_source_id)\n",
    "        elif prop_source !=\"\": # means propvalue_2nd\n",
    "            self.hook_2ndprop_into_props(prop_object,prop_source = prop_source)\n",
    "\n",
    "        else:\n",
    "            # hook directly into the entity object\n",
    "            self.hook_prop_into_props(prop_object)\n",
    "\n",
    "    def hook_ref_object(self, ref_legacyID, input_value, prop_source=\"\",  origin=\"\"):\n",
    "\n",
    "        # allowed entities in ref_legacyID\n",
    "        assert \"R\" in ref_legacyID, f\"Unknown input, R legacyId expected? {ref_legacyID}, {origin}\"\n",
    "\n",
    "        #modify value, so the value object is created\n",
    "        input_value = \"~V~\"+input_value\n",
    "\n",
    "        ref_resource_id = self.get_entity_id(ref_legacyID, origin=origin)\n",
    "        ref_value_id = self.get_entity_id(input_value, origin=origin)\n",
    "\n",
    "        # make IReference object\n",
    "        ref_object = self.make_ref_object(ref_resource_id, ref_value_id)\n",
    "\n",
    "        self.hook_ref_into_refs(ref_object)\n",
    "\n",
    "\n",
    "    def hook_prop_into_props(self,prop_object):\n",
    "        self.entity['props'].append(prop_object)\n",
    "\n",
    "    def hook_ref_into_refs(self,ref_object):\n",
    "        self.entity['references'].append(ref_object)\n",
    "\n",
    "    def hook_2ndprop_into_props(self, prop_object, prop_source = \"\", prop_source_id = \"\", prop_source_field = \"\"):  # identification by concept id\n",
    "\n",
    "        # recognition based on the prop_source_field\n",
    "        if prop_source_field != \"\":\n",
    "\n",
    "            # count, value in enumerate(values)\n",
    "            for count, po in enumerate(self.entity['props']):\n",
    "                # po['id']\n",
    "                # find whether this id is registered under the prop_source_field within this entity frame\n",
    "                candidate_prop_objects = tables['props'][(tables['props']['legacyId'] ==self.entity['legacyId']) & (tables['props']['original_field'] == prop_source_field)]\n",
    "\n",
    "                logger.info(f\"2nprop: {len(candidate_prop_objects)}\")\n",
    "                logger.info(f\"2nprop: {len(candidate_prop_objects)} : {candidate_prop_objects.iloc[0]['id']}\")\n",
    "\n",
    "                if len(candidate_prop_objects) > 0:\n",
    "                    for key, row in candidate_prop_objects.iterrows():\n",
    "\n",
    "                        for  po in self.entity['props']:\n",
    "                            if po['id'] == row['id'] and len(po['children']) == 0:\n",
    "                                logger.info(f\"Found prop. {po['id']}. Adding the 2ndprop child.\")\n",
    "                                po['children'].append(prop_object)\n",
    "\n",
    "                else:\n",
    "                    logger.error(f\"Cannot find the proper prob object record. {prop_object} at {prop_source_field}\")\n",
    "\n",
    "        # recognition based on the type concept of the parent prop object\n",
    "        if prop_source != \"\":\n",
    "            keyId = self.get_entity_id(prop_source)\n",
    "            assert is_valid_uuid(keyId), f\"Cannot recognize entity id from {prop_source}\"\n",
    "\n",
    "            # count, value in enumerate(values)\n",
    "            for count, po in enumerate(self.entity['props']):\n",
    "                if po['type']['id'] == keyId and len(po['children']) == 0:  # I am counting on the fact, that if there are relations from multiples, they are processed in the specific right order\n",
    "                   po['children'].append(prop_object)\n",
    "\n",
    "\n",
    "        # recognition based od propobject id\n",
    "        if prop_source_id != \"\":\n",
    "            keyId = prop_source_id\n",
    "            assert is_valid_uuid(keyId), \"Not valid uuid {keyId}\"\n",
    "\n",
    "            for count, po in enumerate(self.entity['props']):\n",
    "                if po['id'] == keyId:  # I am counting on the fact, that if there are relations from multiples, they are processed in the specific right order\n",
    "                   po['children'].append(prop_object)\n",
    "\n",
    "\n",
    "\n",
    "    # method invoker for the INSIDE operation with concrete fields\n",
    "    def update_inside_field(self, field_name, input_value, origin= \"\"):\n",
    "        if input_value != '':\n",
    "\n",
    "            if (\"#\" in input_value or \"~\" in input_value) and field_name!= \"note\" and \"https://docs.\" not in input_value:\n",
    "                self.logger.info(f\"ALERT # or ~ in the input value {input_value}\")\n",
    "\n",
    "            update_operation = \"update_\" + field_name\n",
    "            update_func = getattr(self, update_operation, self.update_generic)\n",
    "            update_func = getattr(self, update_operation, self.update_generic)\n",
    "            update_func(field_name, input_value, origin)\n",
    "        else:\n",
    "            raise Exception(f\"Trying to update {field_name} with empty input value.\")\n",
    "\n",
    "    #########################################################################################################\n",
    "    # the naming of procedures corresponds to the name of the input_table fields,  used for inside operations\n",
    "\n",
    "    def update_label_language(self, field_name, input_value, origin = \"\"):\n",
    "        if input_value in type(self).enum_mapper['language']:\n",
    "            self.entity['language'] = type(self).enum_mapper['language'][input_value]\n",
    "        else:\n",
    "            self.logger.error(f\"Unable to set language in {origin}.\")\n",
    "            self.entity['language'] = input_value # will raise error\n",
    "\n",
    "    def update_status(self, field_name, input_value, origin = \"\"):\n",
    "        if input_value in type(self).enum_mapper['status']:\n",
    "            self.entity['status'] = type(self).enum_mapper['status'][input_value]\n",
    "        else:\n",
    "            self.logger.error(f\"Unable to set status in {origin}.\")\n",
    "            self.entity['status'] = input_value # will raise error\n",
    "\n",
    "    def update_note(self, field_name, input_value, origin = \"\"):\n",
    "        #self.logger.info(f\"Updating note with {input_value}.\")\n",
    "        if \"#\" in input_value:\n",
    "            values = [v.strip() for v in input_value.split(\"#\")]\n",
    "            for v in values:\n",
    "                self.entity['notes'].append(v)\n",
    "        else:\n",
    "            self.entity['notes'].append(input_value)\n",
    "\n",
    "    def update_id(self, field_name, input_value, origin = \"\"):\n",
    "        # self.entity['id'] = input_value\n",
    "        self.entity['id'] = input_value\n",
    "\n",
    "    def update_legacyId(self, field_name, input_value, origin = \"\"):\n",
    "        # logger.info(f\"Trying to set legacyId {type(input_value)}:'{input_value}' {origin}.\")\n",
    "        self.entity['legacyId'] = input_value\n",
    "\n",
    "    def update_label(self, field_name, input_value, origin = \"\"):\n",
    "        self.entity['label'] = input_value\n",
    "\n",
    "    def update_wordnet_lemma_id(self, field_name, input_value, origin = \"\"):\n",
    "        # self.logger.info(f\" wordnet_lemma_id NOT IMPLEMENTED \")\n",
    "        pass\n",
    "\n",
    "    def update_wordnet_synset_id(self, field_name, input_value, origin = \"\"):\n",
    "        # self.logger.info(f\" wordnet_synset_id NOT IMPLEMENTED \")\n",
    "        pass\n",
    "\n",
    "    def update_generic(self, field_name, input_value, origin = \"\"):\n",
    "        self.entity[field_name] = input_value\n",
    "\n",
    "\n",
    "    def create_audit_record(self, entity_id = '', editor_candidate = '', object = {}):\n",
    "\n",
    "        data_row = self.data_row\n",
    "        if entity_id == '':\n",
    "            entity_id = self.entity['id']\n",
    "        if object == {}:\n",
    "            object = self.entity  # should be run at the end of the entity making, to the object is \"full\"\n",
    "\n",
    "        # date =  {\n",
    "        #     \"$reql_type$\": \"TIME\",\n",
    "        #      \"epoch_time\": time.time(),\n",
    "        #      \"timezone\": \"+00:00\"\n",
    "        # }\n",
    "        date =  datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")+\"Z\" #2021-12-05T19:10:15.739Z\n",
    "\n",
    "        if editor_candidate == '' and 'editor' in data_row.keys():\n",
    "            editor_candidate = data_row['editor']\n",
    "        if editor_candidate in editors.keys():\n",
    "            editor_id = editors[editor_candidate]\n",
    "        else:\n",
    "            editor_id = editors['David Zbíral']\n",
    "\n",
    "        import_audit = IOF.make('IAudit')\n",
    "        import_audit['id'] = get_uuid_id()\n",
    "        import_audit['entityId'] = entity_id\n",
    "        import_audit['user'] =  editors['import']\n",
    "        import_audit['date'] = date\n",
    "        import_audit['changes'] = object\n",
    "\n",
    "        editor_audit = IOF.make('IAudit')\n",
    "        editor_audit['id'] = get_uuid_id()\n",
    "        editor_audit['entityId'] = entity_id\n",
    "        editor_audit['user'] =  editor_id\n",
    "        editor_audit['date'] = date\n",
    "        editor_audit['changes'] = object\n",
    "\n",
    "        # audits.append(import_audit)\n",
    "        audits.append(editor_audit)\n",
    "\n",
    "\n",
    "class TerritoryEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, data_row, logger = d.logger):\n",
    "        EntityMapper.__init__(self,entity_type, data_row, logger)\n",
    "\n",
    "class ConceptEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, data_row, logger = d.logger):\n",
    "        EntityMapper.__init__(self,entity_type, data_row, logger)\n",
    "\n",
    "class ActionEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, data_row, logger = d.logger,):\n",
    "        EntityMapper.__init__(self,entity_type, data_row, logger)\n",
    "\n",
    "    def update_subject_entity_type(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: subject_entity_type\")\n",
    "        entities = [e.strip() for e in value.split(\"|\")]\n",
    "        for e in entities:\n",
    "            if e in self.valid_entity_classes:\n",
    "                self.entity['data']['entities']['s'].append(e)\n",
    "            elif e == \"*\":\n",
    "                self.entity['data']['entities']['s'] = self.valid_entity_classes\n",
    "            else:\n",
    "                logger.error(f\"Non valid entity processed: {e} while AP.update_subject_entity_type().\")\n",
    "\n",
    "    def update_subject_valency(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: subject_valency\")\n",
    "        self.entity['data']['valencies']['s'] = value\n",
    "\n",
    "    def update_actant1_entity_type(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: actant1_entity_type\")\n",
    "        entities = [e.strip() for e in value.split(\"|\")]\n",
    "        for e in entities:\n",
    "            if e in self.valid_entity_classes:\n",
    "                self.entity['data']['entities']['a1'].append(e)\n",
    "            elif e == \"*\":\n",
    "                self.entity['data']['entities']['a1'] = self.valid_entity_classes\n",
    "            else:\n",
    "                logger.error(f\"Non valid entity processed: {e} while AP.update_actant1_entity_type().\")\n",
    "\n",
    "    def update_actant2_entity_type(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: actant2_entity_type\")\n",
    "        entities = [e.strip() for e in value.split(\"|\")]\n",
    "        for e in entities:\n",
    "            if e in self.valid_entity_classes:\n",
    "                self.entity['data']['entities']['a2'].append(e)\n",
    "            elif e == \"*\":\n",
    "                self.entity['data']['entities']['a2'] = self.valid_entity_classes\n",
    "            else:\n",
    "                logger.error(f\"Non valid entity processed: {e} while AP.update_actant2_entity_type().\")\n",
    "\n",
    "    def update_actant1_valency(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: actant1_valency\")\n",
    "        self.entity['data']['valencies']['a1'] = value\n",
    "\n",
    "    def update_actant2_valency(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: actant1_valency\")\n",
    "        self.entity['data']['valencies']['a2'] = value\n",
    "\n",
    "\n",
    "class ResourceEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, data_row, logger = d.logger):\n",
    "        EntityMapper.__init__(self,entity_type, data_row, logger)\n",
    "\n",
    "    def update_url(self, operation, value, entity_mapper):\n",
    "        # self.logger.info(f\"AP custom field: subject_entity_type\")\n",
    "        self.entity['data']['url'] = value\n",
    "\n",
    "\n",
    "class ObjectEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, data_row, logger = d.logger):\n",
    "        EntityMapper.__init__(self,entity_type, data_row, logger)\n",
    "\n",
    "class EventEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, data_row, logger = d.logger):\n",
    "        EntityMapper.__init__(self,entity_type, data_row, logger)\n",
    "\n",
    "class LocationEntityMapper(EntityMapper):\n",
    "    def __init__(self,entity_type, data_row, logger = d.logger):\n",
    "        EntityMapper.__init__(self,entity_type, data_row, logger)\n",
    "\n",
    "\n",
    "class EntityMapperFactory:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def make(self, entity_name, data_row):\n",
    "        if 'ITerritory' == entity_name:\n",
    "            return TerritoryEntityMapper(entity_name, data_row)\n",
    "        elif 'IAction' == entity_name:\n",
    "            return ActionEntityMapper(entity_name, data_row)\n",
    "        elif 'IConcept' == entity_name:\n",
    "            return ConceptEntityMapper(entity_name,data_row)\n",
    "        elif 'IResource' == entity_name:\n",
    "            return ResourceEntityMapper(entity_name, data_row)\n",
    "        elif 'IObject' == entity_name:\n",
    "            return ObjectEntityMapper(entity_name, data_row)\n",
    "        elif 'IEvent' == entity_name:\n",
    "            return EventEntityMapper(entity_name, data_row)\n",
    "        elif 'ILocation' == entity_name:\n",
    "            return LocationEntityMapper(entity_name, data_row)\n",
    "\n",
    "        else:\n",
    "            logger.warning(f\"Unrecognized entity class in entity mapper. Is this right?\")\n",
    "            return EntityMapper(entity_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CONTROL CLASS\n",
    "class ParseController():\n",
    "\n",
    "    def __init__(self, entity_list: [], keyword_row_id = 3,  logger = d.logger):\n",
    "        self.entity_list = entity_list\n",
    "        self.logger = logger\n",
    "        self.parsers = {}\n",
    "        self.js_objects = []\n",
    "\n",
    "        for e in self.entity_list:\n",
    "            if 'texts' in e:\n",
    "                self.parsers[e] = TextParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "            elif 'actions' in e:\n",
    "                self.parsers[e] = ActionParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "            elif 'concepts' in e:\n",
    "                self.parsers[e] = ConceptParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "            elif 'resources' in e:\n",
    "                self.parsers[e] = ResourceParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "            elif 'manuscripts' in e:\n",
    "                self.parsers[e] = ManuscriptParser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "            else:\n",
    "                self.logger.warning(f\"Coming to basic Parser entity - strange '{e}' {type(e)}.\")\n",
    "                self.parsers[e] = Parser(e, header_df = header_infos[e], table_df = tables[e], keyword_row_id = keyword_row_id, logger = logger)\n",
    "\n",
    "    def load_json_objects(self):\n",
    "        for name, p in self.parsers.items():\n",
    "            self.js_objects = self.js_objects + p.js_objects\n",
    "\n",
    "    def parse(self):\n",
    "        for name, p in self.parsers.items():\n",
    "            p.parse_rows()\n",
    "\n",
    "\n",
    "# WORKER CLASS\n",
    "class Parser():\n",
    "    EMP = EntityMapperFactory()\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        self.name = name\n",
    "        self.logname = name.upper()\n",
    "        self.input_header_df = header_df\n",
    "        self.input_table_df = table_df\n",
    "        self.prepared_table = pd.DataFrame()\n",
    "        self.keyword_row_id =  keyword_row_id\n",
    "        self.columns = self.input_header_df.columns.tolist()\n",
    "\n",
    "        self.parsing_instruction = {}\n",
    "        self.oper_columns = {'discard':[],'inside':[],'special':[],'unknown':[],'propvalue':[],'propvalue_2nd':[],\"dependent\":[],\"proptype\":[], \"reference\":[]}\n",
    "        self.logger = logger\n",
    "\n",
    "        # parsed json data holder\n",
    "        self.js_objects = []\n",
    "\n",
    "        # RUN\n",
    "        self.process_header_instructions()\n",
    "        self.prepare_input_table()\n",
    "\n",
    "    # \"parsing\" instructions\n",
    "    def process_header_instructions(self) -> (pd.DataFrame, pd.DataFrame):\n",
    "        keyword_row = self.input_header_df.iloc[self.keyword_row_id]\n",
    "        prop_type_row = self.input_header_df.iloc[self.keyword_row_id - 1]\n",
    "        source_node_row = self.input_header_df.iloc[self.keyword_row_id - 2]\n",
    "\n",
    "        log_uncertain_instructions = []\n",
    "\n",
    "        for c in self.columns:\n",
    "            instruction_candidate = str(keyword_row.at[c]).strip()\n",
    "            prop_type_candidate = str(prop_type_row.at[c]).strip()\n",
    "            source_node_candidate = str(source_node_row.at[c]).strip()\n",
    "\n",
    "            if c == '':\n",
    "                self.logger.error(f\"{self.logname} There is empty column in the dataset.\")\n",
    "                raise Exception(f\"{self.logname} There is empty column in the dataset.\")\n",
    "\n",
    "            if \"?\" in instruction_candidate or \"?\" in prop_type_candidate or \"?\" in source_node_candidate:\n",
    "                log_uncertain_instructions.append(f\"{c.upper()}:{instruction_candidate},{prop_type_candidate},{source_node_candidate}\")\n",
    "                instruction  = {'operation':'discard', 'target': None}\n",
    "                self.oper_columns['discard'].append(c)\n",
    "\n",
    "\n",
    "            # known instructions\n",
    "            if 'discard' in instruction_candidate:\n",
    "                instruction  = {'operation':'discard', 'target': None}\n",
    "                self.oper_columns['discard'].append(c)\n",
    "\n",
    "            elif 'propvalue' ==  instruction_candidate:\n",
    "                prop_type = prop_type_candidate\n",
    "                source_node = source_node_candidate\n",
    "\n",
    "                # test whether is its propvalue proper or dependent (=proptype is dynamic, value from another column)\n",
    "                if prop_type.strip() == \"\":\n",
    "                    self.oper_columns['dependent'].append(c)\n",
    "                    continue # ignoring \"dependent propvalue\"\n",
    "\n",
    "                if \"?\" in prop_type or \"?\"  in source_node:\n",
    "                    instruction = {'operation':'unknown', 'target': None}\n",
    "                    self.oper_columns['unknown'].append(c)\n",
    "                else:\n",
    "                    instruction  = {'operation':'propvalue', 'type': prop_type, 'source':source_node} # source can be ignored, because the iProp object is sitting inside of it\n",
    "                    self.oper_columns['propvalue'].append(c)\n",
    "\n",
    "            elif 'propvalue_2nd' in instruction_candidate:\n",
    "                prop_type = prop_type_candidate\n",
    "                source_node = source_node_candidate\n",
    "                if \"?\" in prop_type or \"?\"  in source_node:\n",
    "                    instruction = {'operation':'unknown', 'target': None}\n",
    "                    self.oper_columns['unknown'].append(c)\n",
    "                else:\n",
    "                    instruction  = {'operation':'propvalue_2nd', 'type': prop_type, 'source':source_node} # source can NOT be ignored, it signals which existing iProp object will hold this iProp object\n",
    "                    self.oper_columns['propvalue_2nd'].append(c)\n",
    "\n",
    "            elif 'special' in instruction_candidate:\n",
    "                # looks for custom functions registered by column name\n",
    "                prop_type = prop_type_candidate\n",
    "                source_node = source_node_candidate\n",
    "                instruction  = {'operation':'special', 'type': prop_type, 'source':source_node}\n",
    "                self.oper_columns['special'].append(c)\n",
    "\n",
    "            elif 'proptype' in instruction_candidate:\n",
    "                instruction  = {'operation':'proptype', 'type': prop_type, 'source':source_node}\n",
    "                #logger.info(f\"here ...{instruction_candidate} {c}\")\n",
    "                self.oper_columns['proptype'].append(c)\n",
    "\n",
    "            elif 'dependent' in instruction_candidate:\n",
    "                # ignore\n",
    "                # the value is solved by another instruction\n",
    "                instruction  = {'operation':'dependent', 'type': prop_type, 'source':source_node}\n",
    "\n",
    "            elif 'inside' in instruction_candidate:\n",
    "                if \"?\" in c:\n",
    "                    instruction = {'operation':'unknown', 'target': None}\n",
    "                    self.oper_columns['unknown'].append(c)\n",
    "                else:\n",
    "                    instruction  = {'operation':'inside', 'target': None}\n",
    "                    if len(prop_type_candidate) > 0:\n",
    "                        instruction  = {'operation':'inside', 'target': prop_type_candidate}\n",
    "\n",
    "                    self.oper_columns['inside'].append(c)\n",
    "\n",
    "            elif \"reference\" in instruction_candidate:\n",
    "                #logger.info(f\"Found instruction 'reference'. {prop_type_candidate}\")\n",
    "                self.oper_columns[\"reference\"].append(c)\n",
    "                instruction  = {'operation':'reference', 'ref_legecy_id':prop_type_candidate}\n",
    "\n",
    "            else:\n",
    "                instruction = {'operation':'unknown', 'target': None}\n",
    "                self.oper_columns['unknown'].append(c)\n",
    "            self.parsing_instruction[c] = instruction\n",
    "\n",
    "        self.logger.info(f\"{self.logname} Uncertain parsing instructions in {len(log_uncertain_instructions)} columns: \" + \" \".join(log_uncertain_instructions) + \".\")\n",
    "        return self.parsing_instruction\n",
    "\n",
    "    def prepare_input_table(self):\n",
    "        ip = self.input_table_df.copy()\n",
    "\n",
    "        # discard  columns with discard and unknown operations\n",
    "        ip.drop(columns=self.oper_columns['discard']+self.oper_columns['unknown'], inplace=True)\n",
    "\n",
    "        self.logger.info(f\"{self.logname} {len(self.oper_columns['discard']+self.oper_columns['unknown'])} columns have been dropped (discard:{len(self.oper_columns['discard'])}, unknown:{len(self.oper_columns['unknown'])}). Table now has {len(ip.columns)} columns, inside:{len(self.oper_columns['inside'])},propvalue:{len(self.oper_columns['propvalue'])}, special:{len(self.oper_columns['special'])}, proptype: {len(self.oper_columns['proptype'])}, dependent:{len(self.oper_columns['dependent'])}, reference:{len(self.oper_columns['reference'])}. Originally {self.input_table_df.shape[1]} columns.\")\n",
    "\n",
    "        self.prepared_table = ip\n",
    "\n",
    "\n",
    "    def prepare_property(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def make_row_object(self, data_row):\n",
    "        class_name = table_to_entity[self.name]\n",
    "        return self.EMP.make(class_name, data_row)\n",
    "\n",
    "    def itemize_valuestring_for_multiples(self, value_with_multiples, origin=\"\") -> []:\n",
    "        values = []\n",
    "        if isinstance(value_with_multiples,str):\n",
    "            parsed_value = value_with_multiples.split('#')\n",
    "            values =  [item.strip() for item in parsed_value]\n",
    "        else:\n",
    "            raise Exception(f\"Expected value to be string. Got {type(value_with_multiples)}. {origin}\")\n",
    "        return values\n",
    "\n",
    "    def parse_rows(self):\n",
    "        self.logger.info(f\"Starting to parse {self.name}.\")\n",
    "\n",
    "        if  self.prepared_table['label'].isnull().any():\n",
    "            self.prepared_table  = self.prepared_table[self.prepared_table['label'].notna()]\n",
    "            self.logger.info(f\"Empty labels found in {self.name} table. (new entities added through parsing process). Adjusting - entities with empty labels not taken into account.\")\n",
    "\n",
    "        for key, row in self.prepared_table.iterrows():\n",
    "            # self.logger.info(f\"{self.name} Processing row {key}\")\n",
    "            entity_mapper = self.make_row_object(row.to_dict())\n",
    "\n",
    "            for name, value in row.items():\n",
    "                if name in self.parsing_instruction:\n",
    "                    operation = self.parsing_instruction[name]\n",
    "                else:\n",
    "                    continue # silently ignore unknown columns\n",
    "                # logger.info(f\"{self.name} Processing columns {name}, with value {value}. Op:{operation}\")\n",
    "\n",
    "                # force string\n",
    "                value = str(value)\n",
    "\n",
    "                if operation['operation'] == 'inside' and value != '' and '?' not in name:\n",
    "                    # logger.info(f\"{self.name} Processing columns {name}, with value {value}. Op:{operation}\")\n",
    "                    if operation['target']:\n",
    "                        name = operation['target']\n",
    "                    entity_mapper.update_inside_field(name,value,operation['operation'] +\">\"+ str(key)+\":\"+str(name)+\":\"+str(value))\n",
    "\n",
    "                if operation['operation'] == 'propvalue' and value != '':\n",
    "                    # logger.info(f\"{self.name} Processing columns {name}, with value {value}. Op:{operation}\")\n",
    "                    prop_type = operation['type']\n",
    "                    if prop_type == '' or 'C' not in prop_type:\n",
    "                        raise Exception(f\"Propvalue does not have prop type defined. C entity-string expected, got {key}, {name}, {value}\")\n",
    "                    for item in self.itemize_valuestring_for_multiples(value):\n",
    "                        entity_mapper.hook_prop_object(prop_type = prop_type, input_value = item, origin = operation['operation'] +\">\"+ str(key)+\":\"+str(name)+\":\"+str(value), field_name = name)\n",
    "\n",
    "                if operation['operation'] == 'propvalue_2nd' and value != '':\n",
    "                    # logger.info(f\"{self.name} Processing columns {name}, with value {value}. Op:{operation}\")\n",
    "                    prop_type = operation['type']\n",
    "                    prop_source_name = operation['source']  # header_name,  we need concept_id\n",
    "                    prop_source_id = self.parsing_instruction[prop_source_name]['type']\n",
    "\n",
    "                    logger.info(f\"{prop_source_name} : {prop_source_id} : {value}\")\n",
    "\n",
    "                    assert 'C' in prop_source_id, f\"Trying to get to the concept of 1st level property to adress 2nd level property.\"\n",
    "\n",
    "                    if prop_type == '' or 'C' not in prop_type:\n",
    "                        raise Exception(f\"Propvalue does not have prop type defined. C entity-string expected, got {key}, {name}, {value}\")\n",
    "\n",
    "                    origin = operation['operation'] +\">\"+ str(key)+\":\"+str(name)+\":\"+str(value)\n",
    "\n",
    "                    for item in self.itemize_valuestring_for_multiples(value, origin=origin):\n",
    "                        entity_mapper.hook_prop_object(prop_type = prop_type, input_value = item, prop_source = prop_source_id, origin = origin, field_name = name, prop_source_field = prop_source_name)\n",
    "\n",
    "                if operation['operation'] == 'special' and value != \"\":\n",
    "                    # logger.info(f\"SPECIAL {operation} {value}\")\n",
    "                    func = getattr(self, 'special_'+name)\n",
    "                    func(operation, value, entity_mapper)\n",
    "\n",
    "\n",
    "\n",
    "                if operation['operation'] == 'reference' and value != \"\":\n",
    "                    # logger.info(f\"SPECIAL {operation} {value}\")\n",
    "                    # func = getattr(self, 'special_'+name)\n",
    "                    # func(operation, value, entity_mapper)\n",
    "\n",
    "                    ref_legecy_id = operation['ref_legecy_id']\n",
    "                    # logger.info(f\"Doing {operation}, {ref_legecy_id}, '{value}'\")\n",
    "                    entity_mapper.hook_ref_object(ref_legacyID = ref_legecy_id, input_value = value, origin = operation['operation'] +\">\"+ \":\"+ name + \" \" +str(value))\n",
    "\n",
    "            self.js_objects.append(entity_mapper.entity)\n",
    "\n",
    "            # make audit records\n",
    "            entity_mapper.create_audit_record()\n",
    "\n",
    "            # break  DEV, checking parsing after first iteration\n",
    "\n",
    "\n",
    "\n",
    "    def special_editor(self, operation, value, entity_mapper, field_name=\"special_editor\"):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TextParser(Parser):\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        Parser.__init__(self, name, header_df, table_df, keyword_row_id, logger)\n",
    "\n",
    "    # special methods for fields, which needs fully individual processing\n",
    "    def special_edition_1(self, operation, value, entity_mapper, field_name=\"edition_1\", ):\n",
    "        # Parse this col. as \"propvalue\" - but you need to generate the target entities since they do not exist. How to do it: for any value, create an R entity with \"label\" = textual value in this col., \"label language\" = English, \"status\" = \"approved\", and \"URL\" = the hyperlink in the formula sitting on the textual value in this col. As usual, ignore NS and NA values (exact match) - do not import anything if the value is NA.\n",
    "        # logger.info(f\"Special edition1 running ...{operation} {value}\")\n",
    "\n",
    "        origin = operation['operation'] +\" \"+ operation['type']+ \">\"+ \":\"+field_name + str(value)\n",
    "        prop_type = operation['type']\n",
    "\n",
    "        # make rentity\n",
    "        # logger.info(f\"Rentity making {value}\")\n",
    "        if \"|\" in value:\n",
    "            data = value.split(\"|\")\n",
    "            label = data[0]\n",
    "            url = data[1]\n",
    "        else:\n",
    "            url = \"\"\n",
    "            label = value\n",
    "            # logger.warning(f\"Expected char | signaling url after label. Got just {value}.\"+origin)\n",
    "\n",
    "        rentity = entity_mapper.make_rentity(label, url, origin=origin)\n",
    "        rentity['language'] = entity_mapper.enum_mapper['language']['English']\n",
    "        entity_mapper.create_audit_record(entity_id=rentity['id'], object=rentity)\n",
    "\n",
    "        entity_mapper.hook_prop_object(prop_type = prop_type, input_value = rentity['id'], origin = origin, field_name=field_name)\n",
    "\n",
    "    def special_edition_2(self, operation, value, entity_mapper):\n",
    "        self.special_edition_1( operation, value, entity_mapper, field_name=\"edition_2\")\n",
    "\n",
    "    def special_edition_3(self, operation, value, entity_mapper):\n",
    "        self.special_edition_1( operation, value, entity_mapper, field_name=\"edition_3\")\n",
    "\n",
    "    def special_creation_event_id(self, operation, value, entity_mapper : EntityMapper, field_name=\"creation_event_id\"):\n",
    "        # Create entities in this col. as new E entities, with (1) the value here as legacy_id, (2) assign (as usual) a new “hash” ID from the db, (3) label of this E: see next col., (4) logical type “definite” (default), (5) label language “English”, (6) status “approved”, and (7) attach to any of those Es the metaprop \"(has) - C0565 “class” - C2642 “creation” (to instantiate the event to its event type = event class).\n",
    "\n",
    "        data = entity_mapper.data_row\n",
    "        origin = operation['operation']+ \" \" + data['legacyId'] + \">\"+ \":\"+field_name +\" \"+ str(value)\n",
    "\n",
    "        event_entity = entity_mapper.make_eentity(data['creation_event_label'], legacyId = value, origin = origin)\n",
    "        event_entity['language'] = entity_mapper.enum_mapper['language']['English']\n",
    "\n",
    "        prop_type_id = entity_mapper.get_entity_id(\"C0565\")\n",
    "        prop_value_id = entity_mapper.get_entity_id(\"C2642\")\n",
    "\n",
    "        # make IProp object\n",
    "        prop_object = entity_mapper.make_prop_object(prop_type_id, prop_value_id)\n",
    "\n",
    "        # hook prop object to the event entity\n",
    "        event_entity['props'].append(prop_object)\n",
    "\n",
    "        # hook the vent event to the territory\n",
    "        entity_mapper.hook_prop_object(prop_type = \"C2642\", input_value = event_entity['id'], origin = origin, field_name=field_name)\n",
    "\n",
    "        # process time_relations\n",
    "        t_relations = [(\n",
    "            data[\"timerelation1_type_conceptified_id\"], data[\"timerelation1_target_id\"]\n",
    "        ), (\n",
    "            data[\"timerelation2_type_conceptified_id\"], data[\"timerelation2_target_id\"]\n",
    "        ),(\n",
    "            data[\"timerelation3_type_conceptified_id\"], data[\"timerelation3_target_id\"]\n",
    "        ), (\n",
    "            data[\"timerelation4_type_conceptified_id\"], data[\"timerelation4_target_id\"]\n",
    "        )]\n",
    "\n",
    "\n",
    "        for o in t_relations:\n",
    "            if len(o[0]) > 0 and len(o[1]) > 0:\n",
    "                prop_type_id = entity_mapper.get_entity_id(o[0], origin = origin)\n",
    "                prop_value_id = entity_mapper.get_entity_id(\"~V~\"+o[1], origin = origin)\n",
    "                prop_object = entity_mapper.make_prop_object(prop_type_id, prop_value_id)\n",
    "                event_entity['props'].append(prop_object)\n",
    "\n",
    "        # create audit record\n",
    "        entity_mapper.create_audit_record(entity_id=event_entity['id'], object=event_entity)\n",
    "\n",
    "\n",
    "    # empty, operation is solved by f above\n",
    "    def special_creation_event_label(self, operation, value, entity_mapper):\n",
    "        pass\n",
    "\n",
    "class ActionParser(Parser):\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        Parser.__init__(self, name, header_df, table_df, keyword_row_id, logger)\n",
    "\n",
    "\n",
    "class ConceptParser(Parser):\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        Parser.__init__(self, name, header_df, table_df, keyword_row_id, logger)\n",
    "\n",
    "    def special_wordnet_synset_id(self, operation, value, entity_mapper,field_name=\"special_wordnet_synset_id\",):\n",
    "        # wordnet_resource_id = R0067\n",
    "        entity_mapper.hook_ref_object(ref_legacyID = \"R0067\", input_value = value, origin = operation['operation'] +\">\"+ \":\"+field_name + \" \" +str(value))\n",
    "\n",
    "\n",
    "class ManuscriptParser(Parser):\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        Parser.__init__(self, name, header_df, table_df, keyword_row_id, logger)\n",
    "\n",
    "    def special_creation_event_id(self, operation, value, entity_mapper : EntityMapper, field_name=\"creation_event_id\"):\n",
    "        # Create entities in this col. as new E entities, with (1) the value here as legacy_id, (2) assign (as usual) a new “hash” ID from the db, (3) label of this E: see next col., (4) logical type “definite” (default), (5) label language “English”, (6) status “approved”, and (7) attach to any of those Es the metaprop \"(has) - C0565 “class” - C2642 “creation” (to instantiate the event to its event type = event class).\n",
    "\n",
    "        origin = operation['operation'] +\">\"+ \":\"+field_name +\" \"+ str(value)\n",
    "\n",
    "        data = entity_mapper.data_row\n",
    "        event_entity = entity_mapper.make_eentity(data['creation_event_label'], legacyId = value, origin = origin)\n",
    "        event_entity['language'] = entity_mapper.enum_mapper['language']['English']\n",
    "\n",
    "        prop_type_id = entity_mapper.get_entity_id(\"C0565\")\n",
    "        prop_value_id = entity_mapper.get_entity_id(\"C2642\")\n",
    "\n",
    "        # make IProp object\n",
    "        prop_object = entity_mapper.make_prop_object(prop_type_id, prop_value_id)\n",
    "\n",
    "\n",
    "        # hook prop object to the event entity\n",
    "        event_entity['props'].append(prop_object)\n",
    "\n",
    "        # hook the vent event to the territory\n",
    "        entity_mapper.hook_prop_object(prop_type = \"C2642\", input_value = event_entity['id'], origin = operation['operation'] +\">\"+ \":\"+field_name + str(value))\n",
    "\n",
    "        # process time_relations\n",
    "        t_relations = [(\n",
    "            data[\"timerelation1_type_conceptified_id\"], data[\"timerelation1_target_id\"]\n",
    "        ), (\n",
    "            data[\"timerelation2_type_conceptified_id\"], data[\"timerelation2_target_id\"]\n",
    "        ),(\n",
    "            data[\"timerelation3_type_conceptified_id\"], data[\"timerelation3_target_id\"]\n",
    "        ), (\n",
    "            data[\"timerelation4_type_conceptified_id\"], data[\"timerelation4_target_id\"]\n",
    "        )]\n",
    "\n",
    "        origin = origin + \"LegacyId: \"+ data['legacyId']\n",
    "        for o in t_relations:\n",
    "            if len(o[0]) > 0 and len(o[1]) > 0:\n",
    "                prop_type_id = entity_mapper.get_entity_id(o[0], origin = origin)\n",
    "                prop_value_id = entity_mapper.get_entity_id(\"~V~\"+o[1], origin = origin)\n",
    "                prop_object = entity_mapper.make_prop_object(prop_type_id, prop_value_id)\n",
    "                event_entity['props'].append(prop_object)\n",
    "\n",
    "        # create audit record\n",
    "        entity_mapper.create_audit_record(entity_id=event_entity['id'], object=event_entity)\n",
    "\n",
    "\n",
    "    # empty, operation is solved by f above\n",
    "    def special_creation_event_label(self, operation, value, entity_mapper):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def special_repository_label(self, operation, value, entity_mapper, field_name = \"repository_label\"):\n",
    "        # For each non-empty, non-NA, non-NS row: (1) generate L entity with label = value in this col., status = “approved”, entity logical type = “definite”, label language = value in the next col. (repository_label_language); (2) append to this L entity a metaprop (has) - C0565 “class” - C2646 “archive or library”, and (3) under the O entity representing the row (i.e. the physical manuscript), add a metaprop which will relate this O to this L entity (repository) through the relation: O - (has) - C2645 “repository” - L in this col.\n",
    "\n",
    "        origin = f\"Making location '{value}' from \" + entity_mapper.data_row['legacyId'] + f\" by field {field_name}.\"\n",
    "\n",
    "        if value != \"\" and value!=\"NA\" and value!=\"N/A\" and value!=\"NS\":  # check value\n",
    "            lentity = entity_mapper.make_lentity(label=value, legacyId=\"L_from_\"+entity_mapper.data_row['legacyId'], origin=origin)\n",
    "\n",
    "            lentity['language'] = entity_mapper.enum_mapper['language'][entity_mapper.data_row['repository_label_language']]\n",
    "\n",
    "            prop_type_id = entity_mapper.get_entity_id(\"C0565\", origin = origin)\n",
    "            prop_value_id = entity_mapper.get_entity_id(\"C2646\", origin = origin)\n",
    "            prop_object = entity_mapper.make_prop_object(prop_type_id, prop_value_id)\n",
    "            lentity['props'].append(prop_object)\n",
    "\n",
    "            # let the manuscript O own the location\n",
    "            entity_mapper.hook_prop_object(prop_type = \"C2645\", input_value = lentity['id'], origin = origin)\n",
    "\n",
    "            # create audit record\n",
    "            entity_mapper.create_audit_record(entity_id=lentity['id'], object=lentity)\n",
    "\n",
    "\n",
    "    # solves the method above\n",
    "    def special_repository_label_language(self, operation, value, entity_mapper):\n",
    "        pass\n",
    "\n",
    "    def special_reproduction_online_url(self, operation, value, entity_mapper, field_name = \"reproduction_online_url\"):\n",
    "        # If non-empty, non-NA, (1) generate an R entity with label \"Reproduction of \" + label of the MS (i.e. value in the B column, status = “approved”, label-language = “English”, url = the URL sitting under the hyperlink value in this cell, and (2) add metaprop to the O entity represented by this row: O - (has) - C1199 “digital reproduction” - the R entity here generated.\n",
    "\n",
    "        origin = f\"Making Resource entity '{value}' from \" + entity_mapper.data_row['legacyId'] + f\" by field {field_name}.\"\n",
    "\n",
    "        if value != \"\" and value!=\"NA\" and value!=\"N/A\" and value!=\"NS\":  # check value\n",
    "\n",
    "            if \"|\" in value:\n",
    "                data = value.split(\"|\")\n",
    "                label = data[0]\n",
    "                url = data[1]\n",
    "\n",
    "                if label == \"link\":\n",
    "                    label = \"Reproduction of \" + entity_mapper.data_row['label']\n",
    "\n",
    "            else:\n",
    "                url = \"\"\n",
    "                label = value\n",
    "                logger.warning(f\"Expected char | signaling url after label. Got just {value}. \"+origin)\n",
    "\n",
    "            rentity = entity_mapper.make_rentity(label, url, origin=origin)\n",
    "            rentity['language'] = entity_mapper.enum_mapper['language']['English']\n",
    "            entity_mapper.hook_prop_object(prop_type = \"C1199\", input_value = rentity['id'], origin = origin)\n",
    "\n",
    "            # create audit record\n",
    "            entity_mapper.create_audit_record(entity_id=rentity['id'], object=rentity)\n",
    "\n",
    "\n",
    "class ResourceParser(Parser):\n",
    "\n",
    "    def __init__(self, name, header_df: pd.DataFrame, table_df: pd.DataFrame, keyword_row_id: int, logger: logger):\n",
    "        Parser.__init__(self, name, header_df, table_df, keyword_row_id, logger)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "audits = [] # null audits\n",
    "additional_entities = [] # null additional entities\n",
    "# making empty table of additional entities\n",
    "tables['values'] = pd.DataFrame(columns=['id','value','origin'])\n",
    "tables['locations'] = pd.DataFrame(columns=['id','value','origin','legacyId'])\n",
    "tables['events'] = pd.DataFrame(columns=['id','value','origin','legacyId'])\n",
    "tables['props'] = pd.DataFrame(columns=['id','type','type_id','value','value_id','original_field','origin','entityId','legacyId'])\n",
    "\n",
    "\n",
    "logger.info(f\"Start\")\n",
    "cp = ParseController(entity_list=['texts','manuscripts','resources','concepts','actions'])\n",
    "#cp = ParseController(entity_list=['concepts'])\n",
    "cp.parse()\n",
    "logger.info(f\"End\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        id   type  \\\n0     a1c87652-6a46-441d-98f4-ba3a3689d32d  C0732   \n1     f0a4b548-dfb0-4a84-9079-1362255cc600  C0335   \n2     7d4f688c-4838-4909-88f7-d4ff7ec1e8d5  C1178   \n3     dc0e5617-8cd1-4d1d-b3ac-e19fe6bb7bfd  C2642   \n4     afc8f51b-1d2c-4965-905b-a27c2e84492d  C1181   \n...                                    ...    ...   \n7566  efc24882-2278-46b4-b0a7-bee68d4fffc6  C3066   \n7567  798d0f39-f6d2-48a0-9383-3415c83b78a3  C0948   \n7568  c4795ed8-e7da-4c88-9157-7949e55fe204  C0949   \n7569  64c15893-3943-4219-978b-729a549e4eb2  C0950   \n7570  d6e8fb1c-91db-4743-b695-13227d46a522  C3066   \n\n                                   type_id  \\\n0     79439adb-bf28-4044-9ca1-f130556386b4   \n1     41739d75-60bd-4f97-b7ec-72d4ec20df8e   \n2     1fe8d0de-a5a8-4e91-b8b7-d8ea8c693b21   \n3     6b3019aa-449d-4c06-b188-40ce009dbe67   \n4     98a9c24f-d41e-4593-9467-225b6d93efbb   \n...                                    ...   \n7566  c1157869-0e76-4e17-8d4f-023026f74bc5   \n7567  03cd0202-0492-43e2-ba6f-87268bbe98a0   \n7568  51aae53e-51da-47bb-82e6-d5c4857f1cec   \n7569  66790dcd-7726-4d2c-b09a-d254403735e5   \n7570  c1157869-0e76-4e17-8d4f-023026f74bc5   \n\n                                     value  \\\n0                                    C0938   \n1                                    C1134   \n2                                    C1138   \n3     f19925b2-0eeb-4a7a-87b6-849860b6b947   \n4                                      M21   \n...                                    ...   \n7566                                 C2633   \n7567                                 C0993   \n7568                                 C0993   \n7569                                 C0991   \n7570                                 C0563   \n\n                                  value_id           original_field  \\\n0     d6f3a74c-8312-41ff-b547-d7d7f852559d              language_id   \n1     e32c1d3a-a2e6-4189-ae6d-f015a8b0040c                 genre_id   \n2     e191ca0c-3624-4748-b6ba-f3cc9be0ef17  milieu_of_provenance_id   \n3     f19925b2-0eeb-4a7a-87b6-849860b6b947        creation_event_id   \n4     45c40ce6-17a2-4ebb-81ec-85363bb8596d     manuscript_witness_1   \n...                                    ...                      ...   \n7566  989f7ab5-a2fe-49fe-b733-90ed6a3b138d      event_equivalent_id   \n7567  a8bb7907-352d-401d-95d0-34973694345c      subject_semantic_id   \n7568  a8bb7907-352d-401d-95d0-34973694345c      actant1_semantic_id   \n7569  fee90ec7-d0b2-43b8-afbe-d9a1ed5fe0f6      actant2_semantic_id   \n7570  cb7b1afc-d997-4de4-8c55-524f8e19e0f8      event_equivalent_id   \n\n                                         origin  \\\n0                 propvalue>0:language_id:C0938   \n1                    propvalue>0:genre_id:C1134   \n2     propvalue>0:milieu_of_provenance_id:C1138   \n3        special T1>:creation_event_id E0001_T1   \n4          propvalue>0:manuscript_witness_1:M21   \n...                                         ...   \n7566    propvalue>448:event_equivalent_id:C2633   \n7567    propvalue>449:subject_semantic_id:C0993   \n7568    propvalue>449:actant1_semantic_id:C0993   \n7569    propvalue>449:actant2_semantic_id:C0991   \n7570    propvalue>449:event_equivalent_id:C0563   \n\n                                  entityId legacyId  \n0     84703e7f-3a89-4a02-89e5-1d64ca9a2e34       T1  \n1     84703e7f-3a89-4a02-89e5-1d64ca9a2e34       T1  \n2     84703e7f-3a89-4a02-89e5-1d64ca9a2e34       T1  \n3     84703e7f-3a89-4a02-89e5-1d64ca9a2e34       T1  \n4     84703e7f-3a89-4a02-89e5-1d64ca9a2e34       T1  \n...                                    ...      ...  \n7566  2c300aaf-ef22-4cd2-b416-4ad83f516ed3    A0452  \n7567  6e06aa07-42e5-425a-adac-fe0f8624d7c9    A0453  \n7568  6e06aa07-42e5-425a-adac-fe0f8624d7c9    A0453  \n7569  6e06aa07-42e5-425a-adac-fe0f8624d7c9    A0453  \n7570  6e06aa07-42e5-425a-adac-fe0f8624d7c9    A0453  \n\n[7571 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>type</th>\n      <th>type_id</th>\n      <th>value</th>\n      <th>value_id</th>\n      <th>original_field</th>\n      <th>origin</th>\n      <th>entityId</th>\n      <th>legacyId</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a1c87652-6a46-441d-98f4-ba3a3689d32d</td>\n      <td>C0732</td>\n      <td>79439adb-bf28-4044-9ca1-f130556386b4</td>\n      <td>C0938</td>\n      <td>d6f3a74c-8312-41ff-b547-d7d7f852559d</td>\n      <td>language_id</td>\n      <td>propvalue&gt;0:language_id:C0938</td>\n      <td>84703e7f-3a89-4a02-89e5-1d64ca9a2e34</td>\n      <td>T1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f0a4b548-dfb0-4a84-9079-1362255cc600</td>\n      <td>C0335</td>\n      <td>41739d75-60bd-4f97-b7ec-72d4ec20df8e</td>\n      <td>C1134</td>\n      <td>e32c1d3a-a2e6-4189-ae6d-f015a8b0040c</td>\n      <td>genre_id</td>\n      <td>propvalue&gt;0:genre_id:C1134</td>\n      <td>84703e7f-3a89-4a02-89e5-1d64ca9a2e34</td>\n      <td>T1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7d4f688c-4838-4909-88f7-d4ff7ec1e8d5</td>\n      <td>C1178</td>\n      <td>1fe8d0de-a5a8-4e91-b8b7-d8ea8c693b21</td>\n      <td>C1138</td>\n      <td>e191ca0c-3624-4748-b6ba-f3cc9be0ef17</td>\n      <td>milieu_of_provenance_id</td>\n      <td>propvalue&gt;0:milieu_of_provenance_id:C1138</td>\n      <td>84703e7f-3a89-4a02-89e5-1d64ca9a2e34</td>\n      <td>T1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dc0e5617-8cd1-4d1d-b3ac-e19fe6bb7bfd</td>\n      <td>C2642</td>\n      <td>6b3019aa-449d-4c06-b188-40ce009dbe67</td>\n      <td>f19925b2-0eeb-4a7a-87b6-849860b6b947</td>\n      <td>f19925b2-0eeb-4a7a-87b6-849860b6b947</td>\n      <td>creation_event_id</td>\n      <td>special T1&gt;:creation_event_id E0001_T1</td>\n      <td>84703e7f-3a89-4a02-89e5-1d64ca9a2e34</td>\n      <td>T1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>afc8f51b-1d2c-4965-905b-a27c2e84492d</td>\n      <td>C1181</td>\n      <td>98a9c24f-d41e-4593-9467-225b6d93efbb</td>\n      <td>M21</td>\n      <td>45c40ce6-17a2-4ebb-81ec-85363bb8596d</td>\n      <td>manuscript_witness_1</td>\n      <td>propvalue&gt;0:manuscript_witness_1:M21</td>\n      <td>84703e7f-3a89-4a02-89e5-1d64ca9a2e34</td>\n      <td>T1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7566</th>\n      <td>efc24882-2278-46b4-b0a7-bee68d4fffc6</td>\n      <td>C3066</td>\n      <td>c1157869-0e76-4e17-8d4f-023026f74bc5</td>\n      <td>C2633</td>\n      <td>989f7ab5-a2fe-49fe-b733-90ed6a3b138d</td>\n      <td>event_equivalent_id</td>\n      <td>propvalue&gt;448:event_equivalent_id:C2633</td>\n      <td>2c300aaf-ef22-4cd2-b416-4ad83f516ed3</td>\n      <td>A0452</td>\n    </tr>\n    <tr>\n      <th>7567</th>\n      <td>798d0f39-f6d2-48a0-9383-3415c83b78a3</td>\n      <td>C0948</td>\n      <td>03cd0202-0492-43e2-ba6f-87268bbe98a0</td>\n      <td>C0993</td>\n      <td>a8bb7907-352d-401d-95d0-34973694345c</td>\n      <td>subject_semantic_id</td>\n      <td>propvalue&gt;449:subject_semantic_id:C0993</td>\n      <td>6e06aa07-42e5-425a-adac-fe0f8624d7c9</td>\n      <td>A0453</td>\n    </tr>\n    <tr>\n      <th>7568</th>\n      <td>c4795ed8-e7da-4c88-9157-7949e55fe204</td>\n      <td>C0949</td>\n      <td>51aae53e-51da-47bb-82e6-d5c4857f1cec</td>\n      <td>C0993</td>\n      <td>a8bb7907-352d-401d-95d0-34973694345c</td>\n      <td>actant1_semantic_id</td>\n      <td>propvalue&gt;449:actant1_semantic_id:C0993</td>\n      <td>6e06aa07-42e5-425a-adac-fe0f8624d7c9</td>\n      <td>A0453</td>\n    </tr>\n    <tr>\n      <th>7569</th>\n      <td>64c15893-3943-4219-978b-729a549e4eb2</td>\n      <td>C0950</td>\n      <td>66790dcd-7726-4d2c-b09a-d254403735e5</td>\n      <td>C0991</td>\n      <td>fee90ec7-d0b2-43b8-afbe-d9a1ed5fe0f6</td>\n      <td>actant2_semantic_id</td>\n      <td>propvalue&gt;449:actant2_semantic_id:C0991</td>\n      <td>6e06aa07-42e5-425a-adac-fe0f8624d7c9</td>\n      <td>A0453</td>\n    </tr>\n    <tr>\n      <th>7570</th>\n      <td>d6e8fb1c-91db-4743-b695-13227d46a522</td>\n      <td>C3066</td>\n      <td>c1157869-0e76-4e17-8d4f-023026f74bc5</td>\n      <td>C0563</td>\n      <td>cb7b1afc-d997-4de4-8c55-524f8e19e0f8</td>\n      <td>event_equivalent_id</td>\n      <td>propvalue&gt;449:event_equivalent_id:C0563</td>\n      <td>6e06aa07-42e5-425a-adac-fe0f8624d7c9</td>\n      <td>A0453</td>\n    </tr>\n  </tbody>\n</table>\n<p>7571 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables['props']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Profiling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "#%reload_ext line_profiler\n",
    "\n",
    "# def profile():\n",
    "#    cp.parsers['concepts'].parse_rows()\n",
    "\n",
    "#%lprun -f EntityMapper.get_entity_id profile()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# %prun -s tottime profile()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# %lprun?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "                                       id  \\\n0    f19925b2-0eeb-4a7a-87b6-849860b6b947   \n1    84f09bea-5201-43ed-8690-0290f67cf969   \n2    7b645446-ad8d-4050-be36-c887cf1742cf   \n3    ecce588a-e277-43b7-bcbf-ce2ca7b2cfe1   \n4    7710ef7e-9738-495d-8220-7dd746e640d3   \n..                                    ...   \n278  4ddf2ac7-2435-48b2-9e4b-e1ef0f9569f3   \n279  85823cd0-fc64-41da-9303-76a0b569f824   \n280  4472ba89-894f-4b00-97a7-71bdf45d0570   \n281  a9863aa0-0c18-497f-8a96-6639a5318a35   \n282  a062a61b-f730-4cdb-b809-875d9fca8b2f   \n\n                                                 value  \\\n0    Creation of ‘Process against Bernard Niort and...   \n1    Creation of ‘Sentences of William Arnold and S...   \n2     Creation of ‘Peter Seila’s Register of Penances’   \n3    Creation of ‘Register FFF of the Carcassonne i...   \n4    Creation of ‘Confirmation of depositions befor...   \n..                                                 ...   \n278  Creation of St Paul-im-Lavanttal, Stift Sankt ...   \n279  Creation of Roma, Biblioteca Casanatense, ms. ...   \n280  Creation of Roma, Archivio Generale dell’Ordin...   \n281  Creation of Wien, Österreichische Nationalbibl...   \n282  Creation of Paris, Bibliothèque nationale de F...   \n\n                                     origin    legacyId  \n0    special T1>:creation_event_id E0001_T1    E0001_T1  \n1    special T2>:creation_event_id E0002_T2    E0002_T2  \n2    special T3>:creation_event_id E0003_T3    E0003_T3  \n3    special T4>:creation_event_id E0004_T4    E0004_T4  \n4    special T5>:creation_event_id E0005_T5    E0005_T5  \n..                                      ...         ...  \n278   special>:creation_event_id E0135_M153  E0135_M153  \n279   special>:creation_event_id E0136_M154  E0136_M154  \n280   special>:creation_event_id E0137_M155  E0137_M155  \n281   special>:creation_event_id E0138_M156  E0138_M156  \n282   special>:creation_event_id E0139_M157  E0139_M157  \n\n[283 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>value</th>\n      <th>origin</th>\n      <th>legacyId</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>f19925b2-0eeb-4a7a-87b6-849860b6b947</td>\n      <td>Creation of ‘Process against Bernard Niort and...</td>\n      <td>special T1&gt;:creation_event_id E0001_T1</td>\n      <td>E0001_T1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>84f09bea-5201-43ed-8690-0290f67cf969</td>\n      <td>Creation of ‘Sentences of William Arnold and S...</td>\n      <td>special T2&gt;:creation_event_id E0002_T2</td>\n      <td>E0002_T2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7b645446-ad8d-4050-be36-c887cf1742cf</td>\n      <td>Creation of ‘Peter Seila’s Register of Penances’</td>\n      <td>special T3&gt;:creation_event_id E0003_T3</td>\n      <td>E0003_T3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ecce588a-e277-43b7-bcbf-ce2ca7b2cfe1</td>\n      <td>Creation of ‘Register FFF of the Carcassonne i...</td>\n      <td>special T4&gt;:creation_event_id E0004_T4</td>\n      <td>E0004_T4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7710ef7e-9738-495d-8220-7dd746e640d3</td>\n      <td>Creation of ‘Confirmation of depositions befor...</td>\n      <td>special T5&gt;:creation_event_id E0005_T5</td>\n      <td>E0005_T5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>278</th>\n      <td>4ddf2ac7-2435-48b2-9e4b-e1ef0f9569f3</td>\n      <td>Creation of St Paul-im-Lavanttal, Stift Sankt ...</td>\n      <td>special&gt;:creation_event_id E0135_M153</td>\n      <td>E0135_M153</td>\n    </tr>\n    <tr>\n      <th>279</th>\n      <td>85823cd0-fc64-41da-9303-76a0b569f824</td>\n      <td>Creation of Roma, Biblioteca Casanatense, ms. ...</td>\n      <td>special&gt;:creation_event_id E0136_M154</td>\n      <td>E0136_M154</td>\n    </tr>\n    <tr>\n      <th>280</th>\n      <td>4472ba89-894f-4b00-97a7-71bdf45d0570</td>\n      <td>Creation of Roma, Archivio Generale dell’Ordin...</td>\n      <td>special&gt;:creation_event_id E0137_M155</td>\n      <td>E0137_M155</td>\n    </tr>\n    <tr>\n      <th>281</th>\n      <td>a9863aa0-0c18-497f-8a96-6639a5318a35</td>\n      <td>Creation of Wien, Österreichische Nationalbibl...</td>\n      <td>special&gt;:creation_event_id E0138_M156</td>\n      <td>E0138_M156</td>\n    </tr>\n    <tr>\n      <th>282</th>\n      <td>a062a61b-f730-4cdb-b809-875d9fca8b2f</td>\n      <td>Creation of Paris, Bibliothèque nationale de F...</td>\n      <td>special&gt;:creation_event_id E0139_M157</td>\n      <td>E0139_M157</td>\n    </tr>\n  </tbody>\n</table>\n<p>283 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables['events']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "     class                                    id  \\\n0        E  f19925b2-0eeb-4a7a-87b6-849860b6b947   \n1        V  bd6024c8-942c-4339-a671-f37bd2ffa874   \n2        V  469fe5a6-912c-4122-af83-ee7060a8a7f6   \n3        R  f759e098-129d-4be1-a2e5-1eeb4ef9c5c7   \n4        V  7b1110a2-6b4d-4da7-afa7-dced0b794184   \n...    ...                                   ...   \n2702     V  727c6dbd-9e25-4d49-b1d1-a061b448aca0   \n2703     V  3efe1a32-fad0-4fa6-bb90-47d213de3c77   \n2704     V  cd3c1cb3-c064-4e86-ab15-9ba4beb9551e   \n2705     V  556f3e2b-9d97-4e5d-96e9-a0571ef7ee71   \n2706     V  84976d96-5d4f-4782-9850-8ba7737e2a67   \n\n                                                  label language detail  \\\n0     Creation of ‘Process against Bernard Niort and...      eng          \n1                                                  1234                   \n2                                               34r-50r                   \n3                                          Douais, 1900      eng          \n4                                                 61-63                   \n...                                                 ...      ...    ...   \n2702                                           00109468                   \n2703                                           00898754                   \n2704                                    greet%2:32:00::                   \n2705                                           01170315                   \n2706                                     dine%2:34:01::                   \n\n                                                   data  \\\n0                                  {'logicalType': '1'}   \n1                                  {'logicalType': '1'}   \n2                                  {'logicalType': '1'}   \n3     {'url': 'https://www.zotero.org/groups/446972/...   \n4                                  {'logicalType': '1'}   \n...                                                 ...   \n2702                               {'logicalType': '1'}   \n2703                               {'logicalType': '1'}   \n2704                               {'logicalType': '1'}   \n2705                               {'logicalType': '1'}   \n2706                               {'logicalType': '1'}   \n\n                                                  props  \\\n0     [{'bundleEnd': False, 'bundleStart': False, 'c...   \n1                                                    []   \n2                                                    []   \n3                                                    []   \n4                                                    []   \n...                                                 ...   \n2702                                                 []   \n2703                                                 []   \n2704                                                 []   \n2705                                                 []   \n2706                                                 []   \n\n                                                  notes status references  \\\n0     [Import batch [development] 2022-05-17 19:49:2...      1         []   \n1     [Import batch [development] 2022-05-17 19:49:2...      1         []   \n2     [Import batch [development] 2022-05-17 19:49:2...      1         []   \n3     [Import batch [development] 2022-05-17 19:49:2...      1         []   \n4     [Import batch [development] 2022-05-17 19:49:2...      1         []   \n...                                                 ...    ...        ...   \n2702  [Import batch [development] 2022-05-17 19:49:2...      1         []   \n2703  [Import batch [development] 2022-05-17 19:49:2...      1         []   \n2704  [Import batch [development] 2022-05-17 19:49:2...      1         []   \n2705  [Import batch [development] 2022-05-17 19:49:2...      1         []   \n2706  [Import batch [development] 2022-05-17 19:49:2...      1         []   \n\n      legacyId  \n0     E0001_T1  \n1          NaN  \n2          NaN  \n3          NaN  \n4          NaN  \n...        ...  \n2702       NaN  \n2703       NaN  \n2704       NaN  \n2705       NaN  \n2706       NaN  \n\n[2707 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>id</th>\n      <th>label</th>\n      <th>language</th>\n      <th>detail</th>\n      <th>data</th>\n      <th>props</th>\n      <th>notes</th>\n      <th>status</th>\n      <th>references</th>\n      <th>legacyId</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>E</td>\n      <td>f19925b2-0eeb-4a7a-87b6-849860b6b947</td>\n      <td>Creation of ‘Process against Bernard Niort and...</td>\n      <td>eng</td>\n      <td></td>\n      <td>{'logicalType': '1'}</td>\n      <td>[{'bundleEnd': False, 'bundleStart': False, 'c...</td>\n      <td>[Import batch [development] 2022-05-17 19:49:2...</td>\n      <td>1</td>\n      <td>[]</td>\n      <td>E0001_T1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>V</td>\n      <td>bd6024c8-942c-4339-a671-f37bd2ffa874</td>\n      <td>1234</td>\n      <td></td>\n      <td></td>\n      <td>{'logicalType': '1'}</td>\n      <td>[]</td>\n      <td>[Import batch [development] 2022-05-17 19:49:2...</td>\n      <td>1</td>\n      <td>[]</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>V</td>\n      <td>469fe5a6-912c-4122-af83-ee7060a8a7f6</td>\n      <td>34r-50r</td>\n      <td></td>\n      <td></td>\n      <td>{'logicalType': '1'}</td>\n      <td>[]</td>\n      <td>[Import batch [development] 2022-05-17 19:49:2...</td>\n      <td>1</td>\n      <td>[]</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>R</td>\n      <td>f759e098-129d-4be1-a2e5-1eeb4ef9c5c7</td>\n      <td>Douais, 1900</td>\n      <td>eng</td>\n      <td></td>\n      <td>{'url': 'https://www.zotero.org/groups/446972/...</td>\n      <td>[]</td>\n      <td>[Import batch [development] 2022-05-17 19:49:2...</td>\n      <td>1</td>\n      <td>[]</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>V</td>\n      <td>7b1110a2-6b4d-4da7-afa7-dced0b794184</td>\n      <td>61-63</td>\n      <td></td>\n      <td></td>\n      <td>{'logicalType': '1'}</td>\n      <td>[]</td>\n      <td>[Import batch [development] 2022-05-17 19:49:2...</td>\n      <td>1</td>\n      <td>[]</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2702</th>\n      <td>V</td>\n      <td>727c6dbd-9e25-4d49-b1d1-a061b448aca0</td>\n      <td>00109468</td>\n      <td></td>\n      <td></td>\n      <td>{'logicalType': '1'}</td>\n      <td>[]</td>\n      <td>[Import batch [development] 2022-05-17 19:49:2...</td>\n      <td>1</td>\n      <td>[]</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2703</th>\n      <td>V</td>\n      <td>3efe1a32-fad0-4fa6-bb90-47d213de3c77</td>\n      <td>00898754</td>\n      <td></td>\n      <td></td>\n      <td>{'logicalType': '1'}</td>\n      <td>[]</td>\n      <td>[Import batch [development] 2022-05-17 19:49:2...</td>\n      <td>1</td>\n      <td>[]</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2704</th>\n      <td>V</td>\n      <td>cd3c1cb3-c064-4e86-ab15-9ba4beb9551e</td>\n      <td>greet%2:32:00::</td>\n      <td></td>\n      <td></td>\n      <td>{'logicalType': '1'}</td>\n      <td>[]</td>\n      <td>[Import batch [development] 2022-05-17 19:49:2...</td>\n      <td>1</td>\n      <td>[]</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2705</th>\n      <td>V</td>\n      <td>556f3e2b-9d97-4e5d-96e9-a0571ef7ee71</td>\n      <td>01170315</td>\n      <td></td>\n      <td></td>\n      <td>{'logicalType': '1'}</td>\n      <td>[]</td>\n      <td>[Import batch [development] 2022-05-17 19:49:2...</td>\n      <td>1</td>\n      <td>[]</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2706</th>\n      <td>V</td>\n      <td>84976d96-5d4f-4782-9850-8ba7737e2a67</td>\n      <td>dine%2:34:01::</td>\n      <td></td>\n      <td></td>\n      <td>{'logicalType': '1'}</td>\n      <td>[]</td>\n      <td>[Import batch [development] 2022-05-17 19:49:2...</td>\n      <td>1</td>\n      <td>[]</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>2707 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is this d72b2bee-47b4-4abc-8c88-793c72ab676d ?\n",
    "\n",
    "pd.DataFrame(additional_entities)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Last checks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"class\": \"A\", \"id\": \"5f2faa40-9293-4077-a2c2-bbe332337807\", \"legacyId\": \"A0168\", \"label\": \"abiurationem recepit\", \"language\": \"lat\", \"detail\": \"received abjuration (from sb)\", \"data\": {\"entities\": {\"a1\": [\"P\", \"G\"], \"a2\": [\"NULL\"], \"s\": [\"P\", \"G\"]}, \"valencies\": {\"a1\": \"\\\"a\\\" + 6 | 2\", \"a2\": \"NULL\", \"s\": \"1\"}}, \"props\": [{\"bundleEnd\": false, \"bundleStart\": false, \"certainty\": \"1\", \"children\": [], \"elvl\": \"1\", \"id\": \"c33b2838-0b03-49f5-80ff-b8b712d9fce3\", \"logic\": \"1\", \"mood\": [], \"moodvariant\": \"1\", \"bundleOperator\": \"a\", \"type\": {\"id\": \"03cd0202-0492-43e2-ba6f-87268bbe98a0\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}, \"value\": {\"id\": \"56b993d8-0020-4455-aac0-93dffe3d6a67\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}}, {\"bundleEnd\": false, \"bundleStart\": false, \"certainty\": \"1\", \"children\": [], \"elvl\": \"1\", \"id\": \"8f6c061c-0392-4090-a038-1ee147ecfa2c\", \"logic\": \"1\", \"mood\": [], \"moodvariant\": \"1\", \"bundleOperator\": \"a\", \"type\": {\"id\": \"51aae53e-51da-47bb-82e6-d5c4857f1cec\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}, \"value\": {\"id\": \"fe8d4516-5503-47d0-9d84-5824e3b97989\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}}, {\"bundleEnd\": false, \"bundleStart\": false, \"certainty\": \"1\", \"children\": [], \"elvl\": \"1\", \"id\": \"7b7da975-9a4c-4f34-b43f-a2abe4f462f1\", \"logic\": \"1\", \"mood\": [], \"moodvariant\": \"1\", \"bundleOperator\": \"a\", \"type\": {\"id\": \"c1157869-0e76-4e17-8d4f-023026f74bc5\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}, \"value\": {\"id\": \"2775f119-a997-4a95-8a13-6fc259f38dcf\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}}, {\"bundleEnd\": false, \"bundleStart\": false, \"certainty\": \"1\", \"children\": [], \"elvl\": \"1\", \"id\": \"ede90f83-bb54-4d43-b555-7d8b242f8b54\", \"logic\": \"1\", \"mood\": [], \"moodvariant\": \"1\", \"bundleOperator\": \"a\", \"type\": {\"id\": \"5ab70fc2-4e29-4663-a68d-a7347938840f\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}, \"value\": {\"id\": \"17e595c2-7fde-4224-963d-abf715e973a7\", \"elvl\": \"1\", \"logic\": \"1\", \"partitivity\": \"1\", \"virtuality\": \"1\"}}], \"notes\": [\"Import batch [development] 2022-05-17 19:49:28.471601\"], \"status\": \"1\", \"references\": []}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(cp.parsers['actions'].js_objects[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 19:55:32,936 INFO There are 2707 additionally created entities (e.g. values, resources ...).\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"There are {len(additional_entities)} additionally created entities (e.g. values, resources ...).\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Output the parsed data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# root data\n",
    "root_territory = \"\"\"\n",
    " {\n",
    "    \"id\": \"T0\",\n",
    "    \"class\": \"T\",\n",
    "    \"data\": { \"parent\": false },\n",
    "    \"label\": \"root\",\n",
    "    \"detail\": \"\",\n",
    "    \"language\": \"lat\",\n",
    "    \"notes\": [],\n",
    "    \"props\": []\n",
    "  }\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 19:55:36,761 INFO END\n"
     ]
    }
   ],
   "source": [
    "# connects outcomes from the individual parsers\n",
    "cp.load_json_objects()\n",
    "\n",
    "# save json object list of the parsed entities from google sheets in the text file\n",
    "with open('../datasets/all-test/new_entities.json', 'w', encoding='utf-8') as f:\n",
    "    #f.write(str(cp.parsers['texts'].js_objects))\n",
    "    json.dump(cp.js_objects, f)\n",
    "\n",
    "# save json object list of the \"newly created entities\" in the text file\n",
    "with open('../datasets/all-test/additional_entities.json', 'w', encoding='utf-8') as f:\n",
    "    #f.write(str(cp.parsers['texts'].js_objects))\n",
    "    json.dump(additional_entities, f)\n",
    "\n",
    "# read  entities.json\n",
    "with open('../datasets/all/entities.json','r') as f:\n",
    "    #entities_content = f.readlines()\n",
    "    entities_content = f.read().replace('\\n', '')\n",
    "\n",
    "# read  entities.json\n",
    "with open('../datasets/all-test/new_entities.json','r') as f:\n",
    "    #entities_content = f.readlines()\n",
    "    new_entities_content = f.read().replace('\\n', '')\n",
    "\n",
    "additional_entities_string = json.dumps(additional_entities)\n",
    "\n",
    "# write new and combine with old test data\n",
    "with open('../datasets/all-test/entities.json','w', encoding='utf-8') as f:\n",
    "    #merge_content = entities_content[0:-1] +  str(cp.parsers['texts'].js_objects)[1:]\n",
    "    merge_content = entities_content[0:-1] +\", \" + additional_entities_string[1:-1]+ \", \" + new_entities_content[1:]\n",
    "    f.write(str(merge_content))\n",
    "\n",
    "# write just the new parse data to the entities json.\n",
    "with open('../datasets/all-parsed/entities.json','w', encoding='utf-8') as f:\n",
    "    #merge_content = entities_content[0:-1] +  str(cp.parsers['texts'].js_objects)[1:]\n",
    "    merge_content =  \"[\" + root_territory + \",\" + additional_entities_string[1:-1]+ \", \" + new_entities_content[1:]\n",
    "    f.write(str(merge_content))\n",
    "\n",
    "\n",
    "# write the audits json.\n",
    "with open('../datasets/all-parsed/audits.json','w', encoding='utf-8') as f:\n",
    "    json.dump(audits, f)\n",
    "\n",
    "\n",
    "logger.info(\"END\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# Log tables google sheet\n",
    "# https://docs.google.com/spreadsheets/d/1UpbJckLrRYTCz7wcEYhhcc-ihJxiUENx1unCCQqE4TE/edit#gid=0\n",
    "\n",
    "\n",
    "# d.open_gsheet(\"logtables\",\"https://docs.google.com/spreadsheets/d/1UpbJckLrRYTCz7wcEYhhcc-ihJxiUENx1unCCQqE4TE\")\n",
    "\n",
    "# d.write_df_to_gsheet(\"logtables\", \"values\", tables[\"values\"])\n",
    "# d.write_df_to_gsheet(\"logtables\", \"resources\", tables[\"resources\"])\n",
    "# d.write_df_to_gsheet(\"logtables\", \"locations\", tables[\"locations\"])\n",
    "# d.write_df_to_gsheet(\"logtables\", \"events\", tables[\"events\"])\n",
    "# d.write_df_to_gsheet(\"logtables\", \"props\", tables[\"props\"])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}